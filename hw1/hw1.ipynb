{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67685b26-f81b-44d0-9396-f93d1e0411b8",
   "metadata": {},
   "source": [
    "# Homework 1 Ahmet Akman\n",
    "This notebook will consist the efforts led by homework 1 and done by Ahmet Akman in the scope of EE449 Course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827a1727-effb-441c-b62e-48db6d9e8312",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f55f231-cd24-4cad-af2a-2ed71677e954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab37ec3-3606-4c09-a4b2-0178a489717f",
   "metadata": {},
   "source": [
    "## Basic Neural Network Construction and Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cbd39f-d9ec-4d5a-b7da-ecb66ddf7a23",
   "metadata": {},
   "source": [
    "### 1.1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490149ad-92d4-4afd-9e5c-4969d39b002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-2, 2, 0.01, dtype = float)\n",
    "tanh_result = (np.exp(2*x)-1) / (np.exp(2*x)+1)\n",
    "sigmoid_result =  1 / (np.exp(-x)+1)\n",
    "ReLU_result = np.maximum(0,x)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(3)\n",
    "ax[0].plot(x,tanh_result)\n",
    "ax[0].set_title(\"Tanh function\")\n",
    "ax[0].set_xlabel(\"x\")\n",
    "ax[0].set_ylabel(\"tanh(x)\")\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].plot(x,sigmoid_result)\n",
    "ax[1].set_title(\"Sigmoid function\")\n",
    "ax[1].set_xlabel(\"x\")\n",
    "ax[1].set_ylabel(\"sigma(x)\")\n",
    "ax[1].grid()\n",
    "\n",
    "ax[2].plot(x,ReLU_result)\n",
    "ax[2].set_title(\"ReLU function\")\n",
    "ax[2].set_xlabel(\"x\")\n",
    "ax[2].set_ylabel(\"ReLU(x)\")\n",
    "ax[2].grid()\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b63d98-a376-46e8-a92b-dea31fc31ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_tanh_result = 1- (np.exp(x)-np.exp(-x))**2 / (np.exp(x) + np.exp(-x))**2\n",
    "p_sigmoid_result =  np.exp(-x) / (np.exp(-x)+1)**2\n",
    "p_ReLU_result = x >= 0 \n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(3)\n",
    "ax[0].plot(x, p_tanh_result)\n",
    "ax[0].set_title(\"Partial derivative of Tanh function\")\n",
    "ax[0].set_xlabel(\"x\")\n",
    "ax[0].set_ylabel(\"tanh(x)\")\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].plot(x, p_sigmoid_result)\n",
    "ax[1].set_title(\"Partial derivative of Sigmoid function\")\n",
    "ax[1].set_xlabel(\"x\")\n",
    "ax[1].set_ylabel(\"sigma(x)\")\n",
    "ax[1].grid()\n",
    "\n",
    "ax[2].plot(x, p_ReLU_result)\n",
    "ax[2].set_title(\"Partial derivative of ReLU function\")\n",
    "ax[2].set_xlabel(\"x\")\n",
    "ax[2].set_ylabel(\"ReLU(x)\")\n",
    "ax[2].grid()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e0612e-d8e2-41d1-88e2-7a14678ad80a",
   "metadata": {},
   "source": [
    "### 1.2\n",
    "- Sigmoid activated MLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6075b359-898c-4900-826d-638d6b53770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_sigmoid:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.bias_hidden = np.zeros((1, self.hidden_size))\n",
    "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.bias_output = np.zeros((1, self.output_size))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return np.exp(x) / (np.exp(x)+1)\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return np.exp(-x) / (np.exp(-x)+1)**2\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Forward pass through the network\n",
    "        self.hidden_output =  self.sigmoid(np.dot(inputs, self.weights_input_hidden) + self.bias_hidden)\n",
    "        self.output =  np.round(np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, inputs, targets, learning_rate):\n",
    "        # Backward pass through the network\n",
    "        # Compute error\n",
    "        output_error = targets - self.output\n",
    "        hidden_error = output_error * self.sigmoid_derivative(self.hidden_output)\n",
    "        # Compute gradients\n",
    "        output_delta = output_error * self.sigmoid_derivative(self.output)\n",
    "        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output)\n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output = self.weights_hidden_output + learning_rate * np.dot(self.hidden_output.T, output_delta)\n",
    "        self.bias_output = self.bias_output + learning_rate * np.sum(output_delta, axis=0, keepdims=True)\n",
    "        self.weights_input_hidden = self.weights_input_hidden + learning_rate * np.dot(inputs.T, hidden_delta)\n",
    "        self.bias_hidden = self.bias_hidden + learning_rate * np.sum(hidden_delta, axis=0, keepdims=True)\n",
    "\n",
    "from utils import part1CreateDataset, part1PlotBoundary\n",
    "x_train, y_train, x_val, y_val = part1CreateDataset(train_samples=1000, val_samples=100, std=0.4)\n",
    "\n",
    "# Define neural network parameters\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "learning_rate = 0.0001\n",
    "# Create neural network\n",
    "nn = MLP_sigmoid(input_size, hidden_size, output_size)\n",
    "# Train the neural network\n",
    "for epoch in range(10000):\n",
    "    # Forward propagation\n",
    "    output = nn.forward(x_train)\n",
    "    # Backpropagation\n",
    "    nn.backward(x_train, y_train, learning_rate)\n",
    "\n",
    "    # Print the loss (MSE) every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean((y_train - output)**2)\n",
    "        print(f'Epoch {epoch}: Loss = {loss}')\n",
    "\n",
    "        \n",
    "# Test the trained neural network\n",
    "y_predict = nn.forward(x_val)\n",
    "print(f'{np.mean(y_predict==y_val)*100} % of test examples classified correctly.')\n",
    "\n",
    "\n",
    "part1PlotBoundary(x_val, y_val, nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311897f6",
   "metadata": {},
   "source": [
    "- Tanh activated MLP. # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b4129",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_tanh:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.bias_hidden = np.zeros((1, self.hidden_size))\n",
    "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.bias_output = np.zeros((1, self.output_size))\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return (np.exp(2*x)-1) / (np.exp(2*x)+1)\n",
    "    def tanh_derivative(self, x):\n",
    "        return 1- (np.exp(x)-np.exp(-x))**2 / (np.exp(x) + np.exp(-x))**2\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Forward pass through the network\n",
    "        self.hidden_output =  self.tanh(np.dot(inputs, self.weights_input_hidden) + self.bias_hidden)\n",
    "        self.output =  np.round(np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, inputs, targets, learning_rate):\n",
    "        # Backward pass through the network\n",
    "        # Compute error\n",
    "        output_error = targets - self.output\n",
    "        hidden_error = output_error * self.tanh_derivative(self.hidden_output)\n",
    "        # Compute gradients\n",
    "        output_delta = output_error * self.tanh_derivative(self.output)\n",
    "        hidden_delta = hidden_error * self.tanh_derivative(self.hidden_output)\n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output = self.weights_hidden_output + learning_rate * np.dot(self.hidden_output.T, output_delta)\n",
    "        self.bias_output = self.bias_output + learning_rate * np.sum(output_delta, axis=0, keepdims=True)\n",
    "        self.weights_input_hidden = self.weights_input_hidden + learning_rate * np.dot(inputs.T, hidden_delta)\n",
    "        self.bias_hidden = self.bias_hidden + learning_rate * np.sum(hidden_delta, axis=0, keepdims=True)\n",
    "\n",
    "from utils import part1CreateDataset, part1PlotBoundary\n",
    "x_train, y_train, x_val, y_val = part1CreateDataset(train_samples=1000, val_samples=100, std=0.4)\n",
    "\n",
    "# Define neural network parameters\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "learning_rate = 0.00001\n",
    "# Create neural network\n",
    "nn = MLP_tanh(input_size, hidden_size, output_size)\n",
    "# Train the neural network\n",
    "for epoch in range(10000):\n",
    "    # Forward propagation\n",
    "    output = nn.forward(x_train)\n",
    "    # Backpropagation\n",
    "    nn.backward(x_train, y_train, learning_rate)\n",
    "\n",
    "    # Print the loss (MSE) every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean((y_train - output)**2)\n",
    "        print(f'Epoch {epoch}: Loss = {loss}')\n",
    "\n",
    "        \n",
    "# Test the trained neural network\n",
    "y_predict = nn.forward(x_val)\n",
    "print(f'{np.mean(y_predict==y_val)*100} % of test examples classified correctly.')\n",
    "\n",
    "\n",
    "part1PlotBoundary(x_val, y_val, nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0e0e80",
   "metadata": {},
   "source": [
    "- ReLU Activated MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ca8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_ReLU:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.bias_hidden = np.zeros((1, self.hidden_size))\n",
    "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.bias_output = np.zeros((1, self.output_size))\n",
    "\n",
    "    def ReLU(self, x):\n",
    "        return np.maximum(0,x)\n",
    "    def ReLU_derivative(self, x):\n",
    "        return x >= 0\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Forward pass through the network\n",
    "        self.hidden_output =  self.ReLU(np.dot(inputs, self.weights_input_hidden) + self.bias_hidden)\n",
    "        self.output =  np.round(np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, inputs, targets, learning_rate):\n",
    "        # Backward pass through the network\n",
    "        # Compute error\n",
    "        output_error = targets - self.output\n",
    "        hidden_error = output_error * self.ReLU_derivative(self.hidden_output)\n",
    "        # Compute gradients\n",
    "        output_delta = output_error * self.ReLU_derivative(self.output)\n",
    "        hidden_delta = hidden_error * self.ReLU_derivative(self.hidden_output)\n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output = self.weights_hidden_output + learning_rate * np.dot(self.hidden_output.T, output_delta)\n",
    "        self.bias_output = self.bias_output + learning_rate * np.sum(output_delta, axis=0, keepdims=True)\n",
    "        self.weights_input_hidden = self.weights_input_hidden + learning_rate * np.dot(inputs.T, hidden_delta)\n",
    "        self.bias_hidden = self.bias_hidden + learning_rate * np.sum(hidden_delta, axis=0, keepdims=True)\n",
    "\n",
    "from utils import part1CreateDataset, part1PlotBoundary\n",
    "x_train, y_train, x_val, y_val = part1CreateDataset(train_samples=1000, val_samples=100, std=0.4)\n",
    "\n",
    "# Define neural network parameters\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "learning_rate = 0.00001\n",
    "# Create neural network\n",
    "nn = MLP_ReLU(input_size, hidden_size, output_size)\n",
    "# Train the neural network\n",
    "for epoch in range(10000):\n",
    "    # Forward propagation\n",
    "    output = nn.forward(x_train)\n",
    "    # Backpropagation\n",
    "    nn.backward(x_train, y_train, learning_rate)\n",
    "\n",
    "    # Print the loss (MSE) every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean((y_train - output)**2)\n",
    "        print(f'Epoch {epoch}: Loss = {loss}')\n",
    "\n",
    "        \n",
    "# Test the trained neural network\n",
    "y_predict = nn.forward(x_val)\n",
    "print(f'{np.mean(y_predict==y_val)*100} % of test examples classified correctly.')\n",
    "\n",
    "\n",
    "part1PlotBoundary(x_val, y_val, nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b73fb3c",
   "metadata": {},
   "source": [
    "### 1.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1039dd76-50a0-4caa-a037-1477b2c12897",
   "metadata": {},
   "source": [
    "## Implementing a Convolutional Layer with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81084835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# implement a 2D convolutional layer using numpy\n",
    "def my_conv2d(input, kernel):\n",
    "    # input shape: [batch size, input_channels, input_height, input_width]\n",
    "    # kernel shape: [output_channels, input_channels, filter_height, filter width]\n",
    "    batch_size, input_channels, input_height, input_width = input.shape\n",
    "    output_channels, input_channels, filter_height, filter_width = kernel.shape\n",
    "    # output shape: [batch size, output_channels, output_height, output_width]\n",
    "    output_height = input_height - filter_height + 1\n",
    "    output_width = input_width - filter_width + 1\n",
    "    output = np.zeros((batch_size, output_channels, output_height, output_width))\n",
    "    for b in range(batch_size):\n",
    "        for oc in range(output_channels):\n",
    "            for ic in range(input_channels):\n",
    "                for i in range(output_height):\n",
    "                    for j in range(output_width):\n",
    "                        output[b, oc, i, j] = np.sum(input[b, ic, i:i+filter_height, j:j+filter_width] * kernel[oc, ic])\n",
    "    return output\n",
    "\n",
    "# input shape: [batch size, input_channels, input_height, input_width]\n",
    "kernel=np.load('data/kernel.npy')\n",
    "\n",
    "for i in range(10):\n",
    "    input=np.load('data/samples_{}.npy'.format(i))\n",
    "    # input shape: [output_channels, input_channels, filter_height, filter width]\n",
    "    out = my_conv2d(input, kernel)\n",
    "    out_check = torch.conv2d(torch.tensor(input).float(), torch.tensor(kernel).float())\n",
    "    np.save('outputs/out_{}.npy'.format(i), out)\n",
    "\n",
    "    from utils import part2Plots\n",
    "    part2Plots(out = out, save_dir='outputs', filename='out_{}'.format(i) )\n",
    "    part2Plots(out = out_check, save_dir='outputs', filename='out_{}_check'.format(i) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b950a2",
   "metadata": {},
   "source": [
    "## 2.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0eab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4720e4-c5cc-4e8e-8a75-76a29c05fa56",
   "metadata": {},
   "source": [
    "## Experimenting ANN Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82643365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fashion MNIST dataset\n",
    "import torchvision\n",
    "\n",
    "#training set\n",
    "train_data = torchvision.datasets.FashionMNIST(root='./data', train=True, download=False, transform= torchvision.transforms.ToTensor())\n",
    "\n",
    "#test set\n",
    "test_data = torchvision.datasets.FashionMNIST(root='./data', train=False, download=False, transform= torchvision.transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db64cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# divide training data into training and validation sets of 0.8 and 0.2 respectively\n",
    "\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [0.8, 0.2])\n",
    "\n",
    "train_generator = torch.utils.data.DataLoader(train_data, batch_size=96, shuffle=True)\n",
    "val_generator = torch.utils.data.DataLoader(val_data, batch_size=96, shuffle=False)\n",
    "test_generator = torch.utils.data.DataLoader(test_data, batch_size=96, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee38fcbf",
   "metadata": {},
   "source": [
    "### mlp_1 case\n",
    "mlp_1 corresponds to FC-32, ReLU + FC10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caa747de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 86.05 %\n",
      "model saved as 'q3_models/model_mlp_1.pty'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type ndarray is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 89\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq3_models/mlp_1_dict.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m---> 89\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(mlp_1_dict, fp)\n",
      "File \u001b[0;32m~/miniconda3/envs/ee449/lib/python3.12/json/__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[1;32m    174\u001b[0m         check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[1;32m    175\u001b[0m         separators\u001b[38;5;241m=\u001b[39mseparators,\n\u001b[1;32m    176\u001b[0m         default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39miterencode(obj)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    180\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
      "File \u001b[0;32m~/miniconda3/envs/ee449/lib/python3.12/json/encoder.py:432\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 432\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ee449/lib/python3.12/json/encoder.py:406\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 406\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ee449/lib/python3.12/json/encoder.py:439\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    438\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[0;32m--> 439\u001b[0m o \u001b[38;5;241m=\u001b[39m _default(o)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ee449/lib/python3.12/json/encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    181\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type ndarray is not JSON serializable"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# example mlp classifier\n",
    "class mlp_1(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(mlp_1, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.FC = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.prediction_layer = torch.nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        hidden = self.FC(x)\n",
    "        relu = self.relu(hidden)\n",
    "        output = self.prediction_layer(relu)\n",
    "        return output\n",
    "\n",
    "# initialize your model\n",
    "model_mlp_1 = mlp_1(784,32,10)\n",
    "\n",
    "# create loss: use cross entropy loss\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "# create optimizer\n",
    "optimizer = torch.optim.Adam(model_mlp_1.parameters(), lr=0.001)\n",
    "# transfer your model to train mode\n",
    "model_mlp_1.train()\n",
    "\n",
    "\n",
    "mlp_1_dict = {\"name\":\"mlp_1\", \"loss_curve\": [],  \"train_acc_curve\": [], \"val_acc_curve\": [], \"test_acc\": 0, 'weights': []}\n",
    "\n",
    "\n",
    "\n",
    "# train the model and save the training loss and validation loss for every 10 batches using model eval mode.s\n",
    "for epoch in range(15):\n",
    "    for batch, (x, y) in enumerate(train_generator):\n",
    "        # forward pass\n",
    "        output = model_mlp_1(x)\n",
    "        # compute loss\n",
    "        loss_val = loss(output, y)\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # backward pass\n",
    "        loss_val.backward()\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "        # print loss\n",
    "        if batch % 10 == 0:\n",
    "            model_mlp_1.eval()\n",
    "            #print('Epoch: {}, Batch: {}, Loss: {}'.format(epoch, batch, loss_val.item()))\n",
    "            mlp_1_dict['loss_curve'].append(loss_val.item())\n",
    "            training_accuracy = torch.mean((torch.argmax(output, dim=1) == y).float())\n",
    "            mlp_1_dict['train_acc_curve'].append(training_accuracy.item())\n",
    "\n",
    "            # validation loss   \n",
    "            validation_accuracy_per_batch = torch.tensor([])\n",
    "            with torch.no_grad():\n",
    "                for val_x, val_y in val_generator:\n",
    "                    val_output = model_mlp_1(val_x)\n",
    "                    #val_loss = loss(val_output, val_y)\n",
    "                    val_accuracy = torch.mean((torch.argmax(val_output, dim=1) == val_y).float())\n",
    "                    try:\n",
    "                        validation_accuracy_per_batch = torch.cat((validation_accuracy_per_batch ,val_accuracy))\n",
    "                    except:\n",
    "                        validation_accuracy_per_batch = val_accuracy\n",
    "            #print('Validation Accuracy: {}'.format(validation_accuracy_per_batch.mean()))\n",
    "            mlp_1_dict['val_acc_curve'].append(validation_accuracy_per_batch.mean().item())\n",
    "            model_mlp_1.train()\n",
    "\n",
    "\n",
    "\n",
    "# test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "for x, y in test_generator:\n",
    "    output = model_mlp_1(x)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    total += y.size(0)\n",
    "    correct += (predicted == y).sum().item()\n",
    "print('Test Accuracy: {} %'.format(100 * correct / total))\n",
    "mlp_1_dict['test_acc'] = 100 * correct / total\n",
    "# save the model as a pty file\n",
    "torch.save(model_mlp_1.state_dict(), 'q3_models/model_mlp_1.pty')\n",
    "print(\"model saved as 'q3_models/model_mlp_1.pty'\")\n",
    "# get the parameters 784x32 layer as numpy array\n",
    "weights_first_layer = model_mlp_1.FC.weight.data.numpy()\n",
    "mlp_1_dict['weights'] = weights_first_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eddd73f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save mlp_1_dict as a pickle file\n",
    "import pickle\n",
    "with open('q3_models/mlp_1_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(mlp_1_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec16186",
   "metadata": {},
   "source": [
    "### mlp_2 case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e63cb725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [38:14<00:00, 152.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 87.03 %\n",
      "model saved as 'q3_models/model_mlp_2.pty'\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "\n",
    "# Load fashion MNIST dataset\n",
    "import torchvision\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#training set\n",
    "train_data = torchvision.datasets.FashionMNIST(root='./data', train=True, download=False, transform= torchvision.transforms.ToTensor())\n",
    "\n",
    "#test set\n",
    "test_data = torchvision.datasets.FashionMNIST(root='./data', train=False, download=False, transform= torchvision.transforms.ToTensor())\n",
    "\n",
    "\n",
    "import torch\n",
    "# divide training data into training and validation sets of 0.8 and 0.2 respectively\n",
    "\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [0.8, 0.2])\n",
    "\n",
    "train_generator = torch.utils.data.DataLoader(train_data, batch_size=96, shuffle=True)\n",
    "val_generator = torch.utils.data.DataLoader(val_data, batch_size=96, shuffle=False)\n",
    "test_generator = torch.utils.data.DataLoader(test_data, batch_size=96, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# example mlp classifier\n",
    "class mlp_2(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size_1, hidden_size_2, num_classes):\n",
    "        super(mlp_2, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.FC1 = torch.nn.Linear(input_size, hidden_size_1)\n",
    "        self.FC2 = torch.nn.Linear(hidden_size_1, hidden_size_2, bias=False)\n",
    "        self.prediction_layer = torch.nn.Linear(hidden_size_2, num_classes)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        hidden1 = self.FC1(x)\n",
    "        relu = self.relu(hidden1)\n",
    "        hidden2 = self.FC2(relu)\n",
    "        output = self.prediction_layer(hidden2)\n",
    "        return output\n",
    "\n",
    "# initialize your model\n",
    "model_mlp_2 = mlp_2(784 ,32 ,64 ,10)\n",
    "\n",
    "# create loss: use cross entropy loss\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "# create optimizer\n",
    "optimizer = torch.optim.Adam(model_mlp_2.parameters(), lr=0.001)\n",
    "# transfer your model to train mode\n",
    "model_mlp_2.train()\n",
    "\n",
    "\n",
    "mlp_2_dict = {\"name\":\"mlp_2\", \"loss_curve\": [],  \"train_acc_curve\": [], \"val_acc_curve\": [], \"test_acc\": 0, 'weights': []}\n",
    "\n",
    "\n",
    "\n",
    "# train the model and save the training loss and validation loss for every 10 batches using model eval mode.s\n",
    "for epoch in tqdm(range(15)):\n",
    "    for batch, (x, y) in enumerate(train_generator):\n",
    "        # forward pass\n",
    "        output = model_mlp_2(x)\n",
    "        # compute loss\n",
    "        loss_val = loss(output, y)\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # backward pass\n",
    "        loss_val.backward()\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "        # print loss\n",
    "        if batch % 10 == 0:\n",
    "            model_mlp_2.eval()\n",
    "            #print('Epoch: {}, Batch: {}, Loss: {}'.format(epoch, batch, loss_val.item()))\n",
    "            mlp_2_dict['loss_curve'].append(loss_val.item())\n",
    "            training_accuracy = torch.mean((torch.argmax(output, dim=1) == y).float())\n",
    "            mlp_2_dict['train_acc_curve'].append(training_accuracy.item())\n",
    "\n",
    "            # validation loss   \n",
    "            validation_accuracy_per_batch = torch.tensor([])\n",
    "            with torch.no_grad():\n",
    "                for val_x, val_y in val_generator:\n",
    "                    val_output = model_mlp_2(val_x)\n",
    "                    #val_loss = loss(val_output, val_y)\n",
    "                    val_accuracy = torch.mean((torch.argmax(val_output, dim=1) == val_y).float())\n",
    "                    try:\n",
    "                        validation_accuracy_per_batch = torch.cat((validation_accuracy_per_batch ,val_accuracy))\n",
    "                    except:\n",
    "                        validation_accuracy_per_batch = val_accuracy\n",
    "            #print('Validation Accuracy: {}'.format(validation_accuracy_per_batch.mean()))\n",
    "            mlp_2_dict['val_acc_curve'].append(validation_accuracy_per_batch.mean().item())\n",
    "            model_mlp_2.train()\n",
    "\n",
    "\n",
    "\n",
    "# test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "for x, y in test_generator:\n",
    "    output = model_mlp_2(x)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    total += y.size(0)\n",
    "    correct += (predicted == y).sum().item()\n",
    "print('Test Accuracy: {} %'.format(100 * correct / total))\n",
    "mlp_2_dict['test_acc'] = 100 * correct / total\n",
    "# save the model as a pty file\n",
    "torch.save(model_mlp_2.state_dict(), 'q3_models/model_mlp_2.pty')\n",
    "print(\"model saved as 'q3_models/model_mlp_2.pty'\")\n",
    "# get the parameters 784x32 layer as numpy array\n",
    "weights_first_layer = model_mlp_2.FC1.weight.data.numpy()\n",
    "mlp_2_dict['weights'] = weights_first_layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0da5c75-9844-4660-87f1-2a8452f0aecc",
   "metadata": {},
   "source": [
    "## Experimenting Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e710d8-a707-44ad-b663-e8ca151e4fe6",
   "metadata": {},
   "source": [
    "## Experimenting Learning Rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
