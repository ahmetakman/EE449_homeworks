{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67685b26-f81b-44d0-9396-f93d1e0411b8",
   "metadata": {},
   "source": [
    "# Homework 1 Ahmet Akman\n",
    "This notebook will consist the efforts led by homework 1 and done by Ahmet Akman in the scope of EE449 Course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827a1727-effb-441c-b62e-48db6d9e8312",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f55f231-cd24-4cad-af2a-2ed71677e954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab37ec3-3606-4c09-a4b2-0178a489717f",
   "metadata": {},
   "source": [
    "## Basic Neural Network Construction and Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cbd39f-d9ec-4d5a-b7da-ecb66ddf7a23",
   "metadata": {},
   "source": [
    "### 1.1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490149ad-92d4-4afd-9e5c-4969d39b002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-2, 2, 0.01, dtype = float)\n",
    "tanh_result = (np.exp(2*x)-1) / (np.exp(2*x)+1)\n",
    "sigmoid_result =  1 / (np.exp(-x)+1)\n",
    "ReLU_result = np.maximum(0,x)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(3)\n",
    "ax[0].plot(x,tanh_result)\n",
    "ax[0].set_title(\"Tanh function\")\n",
    "ax[0].set_xlabel(\"x\")\n",
    "ax[0].set_ylabel(\"tanh(x)\")\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].plot(x,sigmoid_result)\n",
    "ax[1].set_title(\"Sigmoid function\")\n",
    "ax[1].set_xlabel(\"x\")\n",
    "ax[1].set_ylabel(\"sigma(x)\")\n",
    "ax[1].grid()\n",
    "\n",
    "ax[2].plot(x,ReLU_result)\n",
    "ax[2].set_title(\"ReLU function\")\n",
    "ax[2].set_xlabel(\"x\")\n",
    "ax[2].set_ylabel(\"ReLU(x)\")\n",
    "ax[2].grid()\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b63d98-a376-46e8-a92b-dea31fc31ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_tanh_result = 1- (np.exp(x)-np.exp(-x))**2 / (np.exp(x) + np.exp(-x))**2\n",
    "p_sigmoid_result =  np.exp(-x) / (np.exp(-x)+1)**2\n",
    "p_ReLU_result = x >= 0 \n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(3)\n",
    "ax[0].plot(x, p_tanh_result)\n",
    "ax[0].set_title(\"Partial derivative of Tanh function\")\n",
    "ax[0].set_xlabel(\"x\")\n",
    "ax[0].set_ylabel(\"tanh(x)\")\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].plot(x, p_sigmoid_result)\n",
    "ax[1].set_title(\"Partial derivative of Sigmoid function\")\n",
    "ax[1].set_xlabel(\"x\")\n",
    "ax[1].set_ylabel(\"sigma(x)\")\n",
    "ax[1].grid()\n",
    "\n",
    "ax[2].plot(x, p_ReLU_result)\n",
    "ax[2].set_title(\"Partial derivative of ReLU function\")\n",
    "ax[2].set_xlabel(\"x\")\n",
    "ax[2].set_ylabel(\"ReLU(x)\")\n",
    "ax[2].grid()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e0612e-d8e2-41d1-88e2-7a14678ad80a",
   "metadata": {},
   "source": [
    "### 1.2\n",
    "- Sigmoid activated MLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6075b359-898c-4900-826d-638d6b53770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_sigmoid:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.bias_hidden = np.zeros((1, self.hidden_size))\n",
    "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.bias_output = np.zeros((1, self.output_size))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return np.exp(x) / (np.exp(x)+1)\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return np.exp(-x) / (np.exp(-x)+1)**2\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Forward pass through the network\n",
    "        self.hidden_output =  self.sigmoid(np.dot(inputs, self.weights_input_hidden) + self.bias_hidden)\n",
    "        self.output =  np.round(np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, inputs, targets, learning_rate):\n",
    "        # Backward pass through the network\n",
    "        # Compute error\n",
    "        output_error = targets - self.output\n",
    "        hidden_error = output_error * self.sigmoid_derivative(self.hidden_output)\n",
    "        # Compute gradients\n",
    "        output_delta = output_error * self.sigmoid_derivative(self.output)\n",
    "        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output)\n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output = self.weights_hidden_output + learning_rate * np.dot(self.hidden_output.T, output_delta)\n",
    "        self.bias_output = self.bias_output + learning_rate * np.sum(output_delta, axis=0, keepdims=True)\n",
    "        self.weights_input_hidden = self.weights_input_hidden + learning_rate * np.dot(inputs.T, hidden_delta)\n",
    "        self.bias_hidden = self.bias_hidden + learning_rate * np.sum(hidden_delta, axis=0, keepdims=True)\n",
    "\n",
    "from utils import part1CreateDataset, part1PlotBoundary\n",
    "x_train, y_train, x_val, y_val = part1CreateDataset(train_samples=1000, val_samples=100, std=0.4)\n",
    "\n",
    "# Define neural network parameters\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "learning_rate = 0.0001\n",
    "# Create neural network\n",
    "nn = MLP_sigmoid(input_size, hidden_size, output_size)\n",
    "# Train the neural network\n",
    "for epoch in range(10000):\n",
    "    # Forward propagation\n",
    "    output = nn.forward(x_train)\n",
    "    # Backpropagation\n",
    "    nn.backward(x_train, y_train, learning_rate)\n",
    "\n",
    "    # Print the loss (MSE) every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean((y_train - output)**2)\n",
    "        print(f'Epoch {epoch}: Loss = {loss}')\n",
    "\n",
    "        \n",
    "# Test the trained neural network\n",
    "y_predict = nn.forward(x_val)\n",
    "print(f'{np.mean(y_predict==y_val)*100} % of test examples classified correctly.')\n",
    "\n",
    "\n",
    "part1PlotBoundary(x_val, y_val, nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311897f6",
   "metadata": {},
   "source": [
    "- Tanh activated MLP. # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b4129",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_tanh:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.bias_hidden = np.zeros((1, self.hidden_size))\n",
    "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.bias_output = np.zeros((1, self.output_size))\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return (np.exp(2*x)-1) / (np.exp(2*x)+1)\n",
    "    def tanh_derivative(self, x):\n",
    "        return 1- (np.exp(x)-np.exp(-x))**2 / (np.exp(x) + np.exp(-x))**2\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Forward pass through the network\n",
    "        self.hidden_output =  self.tanh(np.dot(inputs, self.weights_input_hidden) + self.bias_hidden)\n",
    "        self.output =  np.round(np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, inputs, targets, learning_rate):\n",
    "        # Backward pass through the network\n",
    "        # Compute error\n",
    "        output_error = targets - self.output\n",
    "        hidden_error = output_error * self.tanh_derivative(self.hidden_output)\n",
    "        # Compute gradients\n",
    "        output_delta = output_error * self.tanh_derivative(self.output)\n",
    "        hidden_delta = hidden_error * self.tanh_derivative(self.hidden_output)\n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output = self.weights_hidden_output + learning_rate * np.dot(self.hidden_output.T, output_delta)\n",
    "        self.bias_output = self.bias_output + learning_rate * np.sum(output_delta, axis=0, keepdims=True)\n",
    "        self.weights_input_hidden = self.weights_input_hidden + learning_rate * np.dot(inputs.T, hidden_delta)\n",
    "        self.bias_hidden = self.bias_hidden + learning_rate * np.sum(hidden_delta, axis=0, keepdims=True)\n",
    "\n",
    "from utils import part1CreateDataset, part1PlotBoundary\n",
    "x_train, y_train, x_val, y_val = part1CreateDataset(train_samples=1000, val_samples=100, std=0.4)\n",
    "\n",
    "# Define neural network parameters\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "learning_rate = 0.00001\n",
    "# Create neural network\n",
    "nn = MLP_tanh(input_size, hidden_size, output_size)\n",
    "# Train the neural network\n",
    "for epoch in range(10000):\n",
    "    # Forward propagation\n",
    "    output = nn.forward(x_train)\n",
    "    # Backpropagation\n",
    "    nn.backward(x_train, y_train, learning_rate)\n",
    "\n",
    "    # Print the loss (MSE) every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean((y_train - output)**2)\n",
    "        print(f'Epoch {epoch}: Loss = {loss}')\n",
    "\n",
    "        \n",
    "# Test the trained neural network\n",
    "y_predict = nn.forward(x_val)\n",
    "print(f'{np.mean(y_predict==y_val)*100} % of test examples classified correctly.')\n",
    "\n",
    "\n",
    "part1PlotBoundary(x_val, y_val, nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0e0e80",
   "metadata": {},
   "source": [
    "- ReLU Activated MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ca8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_ReLU:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.bias_hidden = np.zeros((1, self.hidden_size))\n",
    "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.bias_output = np.zeros((1, self.output_size))\n",
    "\n",
    "    def ReLU(self, x):\n",
    "        return np.maximum(0,x)\n",
    "    def ReLU_derivative(self, x):\n",
    "        return x >= 0\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Forward pass through the network\n",
    "        self.hidden_output =  self.ReLU(np.dot(inputs, self.weights_input_hidden) + self.bias_hidden)\n",
    "        self.output =  np.round(np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, inputs, targets, learning_rate):\n",
    "        # Backward pass through the network\n",
    "        # Compute error\n",
    "        output_error = targets - self.output\n",
    "        hidden_error = output_error * self.ReLU_derivative(self.hidden_output)\n",
    "        # Compute gradients\n",
    "        output_delta = output_error * self.ReLU_derivative(self.output)\n",
    "        hidden_delta = hidden_error * self.ReLU_derivative(self.hidden_output)\n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output = self.weights_hidden_output + learning_rate * np.dot(self.hidden_output.T, output_delta)\n",
    "        self.bias_output = self.bias_output + learning_rate * np.sum(output_delta, axis=0, keepdims=True)\n",
    "        self.weights_input_hidden = self.weights_input_hidden + learning_rate * np.dot(inputs.T, hidden_delta)\n",
    "        self.bias_hidden = self.bias_hidden + learning_rate * np.sum(hidden_delta, axis=0, keepdims=True)\n",
    "\n",
    "from utils import part1CreateDataset, part1PlotBoundary\n",
    "x_train, y_train, x_val, y_val = part1CreateDataset(train_samples=1000, val_samples=100, std=0.4)\n",
    "\n",
    "# Define neural network parameters\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "learning_rate = 0.00001\n",
    "# Create neural network\n",
    "nn = MLP_ReLU(input_size, hidden_size, output_size)\n",
    "# Train the neural network\n",
    "for epoch in range(10000):\n",
    "    # Forward propagation\n",
    "    output = nn.forward(x_train)\n",
    "    # Backpropagation\n",
    "    nn.backward(x_train, y_train, learning_rate)\n",
    "\n",
    "    # Print the loss (MSE) every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean((y_train - output)**2)\n",
    "        print(f'Epoch {epoch}: Loss = {loss}')\n",
    "\n",
    "        \n",
    "# Test the trained neural network\n",
    "y_predict = nn.forward(x_val)\n",
    "print(f'{np.mean(y_predict==y_val)*100} % of test examples classified correctly.')\n",
    "\n",
    "\n",
    "part1PlotBoundary(x_val, y_val, nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b73fb3c",
   "metadata": {},
   "source": [
    "### 1.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1039dd76-50a0-4caa-a037-1477b2c12897",
   "metadata": {},
   "source": [
    "## Implementing a Convolutional Layer with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81084835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# implement a 2D convolutional layer using numpy\n",
    "def my_conv2d(input, kernel):\n",
    "    # input shape: [batch size, input_channels, input_height, input_width]\n",
    "    # kernel shape: [output_channels, input_channels, filter_height, filter width]\n",
    "    batch_size, input_channels, input_height, input_width = input.shape\n",
    "    output_channels, input_channels, filter_height, filter_width = kernel.shape\n",
    "    # output shape: [batch size, output_channels, output_height, output_width]\n",
    "    output_height = input_height - filter_height + 1\n",
    "    output_width = input_width - filter_width + 1\n",
    "    output = np.zeros((batch_size, output_channels, output_height, output_width))\n",
    "    for b in range(batch_size):\n",
    "        for oc in range(output_channels):\n",
    "            for ic in range(input_channels):\n",
    "                for i in range(output_height):\n",
    "                    for j in range(output_width):\n",
    "                        output[b, oc, i, j] = np.sum(input[b, ic, i:i+filter_height, j:j+filter_width] * kernel[oc, ic])\n",
    "    return output\n",
    "\n",
    "# input shape: [batch size, input_channels, input_height, input_width]\n",
    "kernel=np.load('data/kernel.npy')\n",
    "\n",
    "for i in range(10):\n",
    "    input=np.load('data/samples_{}.npy'.format(i))\n",
    "    # input shape: [output_channels, input_channels, filter_height, filter width]\n",
    "    out = my_conv2d(input, kernel)\n",
    "    out_check = torch.conv2d(torch.tensor(input).float(), torch.tensor(kernel).float())\n",
    "    np.save('outputs/out_{}.npy'.format(i), out)\n",
    "\n",
    "    from utils import part2Plots\n",
    "    part2Plots(out = out, save_dir='outputs', filename='out_{}'.format(i) )\n",
    "    part2Plots(out = out_check, save_dir='outputs', filename='out_{}_check'.format(i) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b950a2",
   "metadata": {},
   "source": [
    "## 2.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0eab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4720e4-c5cc-4e8e-8a75-76a29c05fa56",
   "metadata": {},
   "source": [
    "## Experimenting ANN Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82643365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fashion MNIST dataset\n",
    "import torchvision\n",
    "\n",
    "#training set\n",
    "train_data = torchvision.datasets.FashionMNIST(root='./data', train=True, download=False, transform= torchvision.transforms.ToTensor())\n",
    "\n",
    "#test set\n",
    "test_data = torchvision.datasets.FashionMNIST(root='./data', train=False, download=False, transform= torchvision.transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db64cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# divide training data into training and validation sets of 0.8 and 0.2 respectively\n",
    "\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [0.8, 0.2])\n",
    "\n",
    "train_generator = torch.utils.data.DataLoader(train_data, batch_size=96, shuffle=True)\n",
    "val_generator = torch.utils.data.DataLoader(val_data, batch_size=96, shuffle=False)\n",
    "test_generator = torch.utils.data.DataLoader(test_data, batch_size=96, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee38fcbf",
   "metadata": {},
   "source": [
    "### mlp_1 case\n",
    "mlp_1 corresponds to FC-32, ReLU + FC10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa747de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# example mlp classifier\n",
    "class mlp_1(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(mlp_1, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.FC = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.prediction_layer = torch.nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        hidden = self.FC(x)\n",
    "        relu = self.relu(hidden)\n",
    "        output = self.prediction_layer(relu)\n",
    "        return output\n",
    "\n",
    "# initialize your model\n",
    "model_mlp_1 = mlp_1(784,32,10)\n",
    "if torch.cuda.is_available():\n",
    "    model_mlp_1 = model_mlp_1.cuda()\n",
    "\n",
    "# create loss: use cross entropy loss\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "# create optimizer\n",
    "optimizer = torch.optim.Adam(model_mlp_1.parameters(), lr=0.001)\n",
    "# transfer your model to train mode\n",
    "model_mlp_1.train()\n",
    "\n",
    "\n",
    "mlp_1_dict = {\"name\":\"mlp_1\", \"loss_curve\": [],  \"train_acc_curve\": [], \"val_acc_curve\": [], \"test_acc\": 0, 'weights': []}\n",
    "\n",
    "\n",
    "\n",
    "# train the model and save the training loss and validation loss for every 10 batches using model eval mode.s\n",
    "for epoch in range(15):\n",
    "    for batch, (x, y) in enumerate(train_generator):\n",
    "        # forward pass\n",
    "        output = model_mlp_1(x)\n",
    "        # compute loss\n",
    "        loss_val = loss(output, y)\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # backward pass\n",
    "        loss_val.backward()\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "        # print loss\n",
    "        if batch % 10 == 0:\n",
    "            model_mlp_1.eval()\n",
    "            #print('Epoch: {}, Batch: {}, Loss: {}'.format(epoch, batch, loss_val.item()))\n",
    "            mlp_1_dict['loss_curve'].append(loss_val.item())\n",
    "            training_accuracy = torch.mean((torch.argmax(output, dim=1) == y).float())\n",
    "            mlp_1_dict['train_acc_curve'].append(training_accuracy.item())\n",
    "\n",
    "            # validation loss   \n",
    "            validation_accuracy_per_batch = torch.tensor([])\n",
    "            with torch.no_grad():\n",
    "                for val_x, val_y in val_generator:\n",
    "                    val_output = model_mlp_1(val_x)\n",
    "                    #val_loss = loss(val_output, val_y)\n",
    "                    val_accuracy = torch.mean((torch.argmax(val_output, dim=1) == val_y).float())\n",
    "                    try:\n",
    "                        validation_accuracy_per_batch = torch.cat((validation_accuracy_per_batch ,val_accuracy))\n",
    "                    except:\n",
    "                        validation_accuracy_per_batch = val_accuracy\n",
    "            #print('Validation Accuracy: {}'.format(validation_accuracy_per_batch.mean()))\n",
    "            mlp_1_dict['val_acc_curve'].append(validation_accuracy_per_batch.mean().item())\n",
    "            model_mlp_1.train()\n",
    "\n",
    "\n",
    "\n",
    "# test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "for x, y in test_generator:\n",
    "    output = model_mlp_1(x)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    total += y.size(0)\n",
    "    correct += (predicted == y).sum().item()\n",
    "print('Test Accuracy: {} %'.format(100 * correct / total))\n",
    "mlp_1_dict['test_acc'] = 100 * correct / total\n",
    "# save the model as a pty file\n",
    "torch.save(model_mlp_1.state_dict(), 'q3_models/model_mlp_1.pty')\n",
    "print(\"model saved as 'q3_models/model_mlp_1.pty'\")\n",
    "# get the parameters 784x32 layer as numpy array\n",
    "weights_first_layer = model_mlp_1.FC.weight.data.numpy()\n",
    "mlp_1_dict['weights'] = weights_first_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddd73f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save mlp_1_dict as a pickle file\n",
    "import pickle\n",
    "with open('q3_models/mlp_1_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(mlp_1_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec16186",
   "metadata": {},
   "source": [
    "### mlp_2 case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63cb725",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "# Load fashion MNIST dataset\n",
    "import torchvision\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#training set\n",
    "train_data = torchvision.datasets.FashionMNIST(root='./data', train=True, download=False, transform= torchvision.transforms.ToTensor())\n",
    "\n",
    "#test set\n",
    "test_data = torchvision.datasets.FashionMNIST(root='./data', train=False, download=False, transform= torchvision.transforms.ToTensor())\n",
    "\n",
    "\n",
    "import torch\n",
    "# divide training data into training and validation sets of 0.8 and 0.2 respectively\n",
    "\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [0.8, 0.2])\n",
    "\n",
    "train_generator = torch.utils.data.DataLoader(train_data, batch_size=96, shuffle=True)\n",
    "val_generator = torch.utils.data.DataLoader(val_data, batch_size=96, shuffle=False)\n",
    "test_generator = torch.utils.data.DataLoader(test_data, batch_size=96, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# example mlp classifier\n",
    "class mlp_2(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size_1, hidden_size_2, num_classes):\n",
    "        super(mlp_2, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.FC1 = torch.nn.Linear(input_size, hidden_size_1)\n",
    "        self.FC2 = torch.nn.Linear(hidden_size_1, hidden_size_2, bias=False)\n",
    "        self.prediction_layer = torch.nn.Linear(hidden_size_2, num_classes)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        hidden1 = self.FC1(x)\n",
    "        relu = self.relu(hidden1)\n",
    "        hidden2 = self.FC2(relu)\n",
    "        output = self.prediction_layer(hidden2)\n",
    "        return output\n",
    "\n",
    "# initialize your model\n",
    "model_mlp_2 = mlp_2(784 ,32 ,64 ,10)\n",
    "if torch.cuda.is_available():\n",
    "    model_mlp_2 = model_mlp_2.cuda()\n",
    "\n",
    "\n",
    "# create loss: use cross entropy loss\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "# create optimizer\n",
    "optimizer = torch.optim.Adam(model_mlp_2.parameters(), lr=0.001)\n",
    "# transfer your model to train mode\n",
    "model_mlp_2.train()\n",
    "\n",
    "\n",
    "mlp_2_dict = {\"name\":\"mlp_2\", \"loss_curve\": [],  \"train_acc_curve\": [], \"val_acc_curve\": [], \"test_acc\": 0, 'weights': []}\n",
    "\n",
    "\n",
    "\n",
    "# train the model and save the training loss and validation loss for every 10 batches using model eval mode.s\n",
    "for epoch in tqdm(range(15)):\n",
    "    for batch, (x, y) in enumerate(train_generator):\n",
    "        # forward pass\n",
    "        output = model_mlp_2(x)\n",
    "        # compute loss\n",
    "        loss_val = loss(output, y)\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # backward pass\n",
    "        loss_val.backward()\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "        # print loss\n",
    "        if batch % 10 == 0:\n",
    "            model_mlp_2.eval()\n",
    "            #print('Epoch: {}, Batch: {}, Loss: {}'.format(epoch, batch, loss_val.item()))\n",
    "            mlp_2_dict['loss_curve'].append(loss_val.item())\n",
    "            training_accuracy = torch.mean((torch.argmax(output, dim=1) == y).float())\n",
    "            mlp_2_dict['train_acc_curve'].append(training_accuracy.item())\n",
    "\n",
    "            # validation loss   \n",
    "            validation_accuracy_per_batch = torch.tensor([])\n",
    "            with torch.no_grad():\n",
    "                for val_x, val_y in val_generator:\n",
    "                    val_output = model_mlp_2(val_x)\n",
    "                    #val_loss = loss(val_output, val_y)\n",
    "                    val_accuracy = torch.mean((torch.argmax(val_output, dim=1) == val_y).float())\n",
    "                    try:\n",
    "                        validation_accuracy_per_batch = torch.cat((validation_accuracy_per_batch ,val_accuracy))\n",
    "                    except:\n",
    "                        validation_accuracy_per_batch = val_accuracy\n",
    "            #print('Validation Accuracy: {}'.format(validation_accuracy_per_batch.mean()))\n",
    "            mlp_2_dict['val_acc_curve'].append(validation_accuracy_per_batch.mean().item())\n",
    "            model_mlp_2.train()\n",
    "\n",
    "\n",
    "\n",
    "# test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "for x, y in test_generator:\n",
    "    output = model_mlp_2(x)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    total += y.size(0)\n",
    "    correct += (predicted == y).sum().item()\n",
    "print('Test Accuracy: {} %'.format(100 * correct / total))\n",
    "mlp_2_dict['test_acc'] = 100 * correct / total\n",
    "# save the model as a pty file\n",
    "torch.save(model_mlp_2.state_dict(), 'q3_models/model_mlp_2.pty')\n",
    "print(\"model saved as 'q3_models/model_mlp_2.pty'\")\n",
    "# get the parameters 784x32 layer as numpy array\n",
    "weights_first_layer = model_mlp_2.FC1.weight.data.numpy()\n",
    "mlp_2_dict['weights'] = weights_first_layer\n",
    "\n",
    "\n",
    "# save mlp_2_dict as a pickle file\n",
    "import pickle\n",
    "with open('q3_models/mlp_2_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(mlp_2_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c224c7b3",
   "metadata": {},
   "source": [
    "### CNN3\n",
    "Where Conv - 3x3x16 Relu\n",
    "Conv - 5x5x8 Relı, MaxPool 2x2\n",
    "Conv - 7x7x16 MaxPool 2x2\n",
    "Fully connected layer of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b328356",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "# Load fashion MNIST dataset\n",
    "import torchvision\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#training set\n",
    "train_data = torchvision.datasets.FashionMNIST(root='./data', train=True, download=False, transform= torchvision.transforms.ToTensor())\n",
    "\n",
    "#test set\n",
    "test_data = torchvision.datasets.FashionMNIST(root='./data', train=False, download=False, transform= torchvision.transforms.ToTensor())\n",
    "\n",
    "\n",
    "import torch\n",
    "# divide training data into training and validation sets of 0.8 and 0.2 respectively\n",
    "\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [0.8, 0.2])\n",
    "\n",
    "train_generator = torch.utils.data.DataLoader(train_data, batch_size=96, shuffle=True)\n",
    "val_generator = torch.utils.data.DataLoader(val_data, batch_size=96, shuffle=False)\n",
    "test_generator = torch.utils.data.DataLoader(test_data, batch_size=96, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# example cnn_3 classifier\n",
    "class cnn_3(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(cnn_3, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.Conv1 = torch.nn.Conv2d(1 ,16 ,3, stride=1, padding=1)\n",
    "        self.MaxPool = torch.nn.MaxPool2d(2, stride=2)\n",
    "        self.Conv2 = torch.nn.Conv2d(16, 8, 5,  stride=1, padding=1)\n",
    "        self.Conv3 = torch.nn.Conv2d(8, 16, 7, stride=1, padding=1)\n",
    "        self.prediction_layer = torch.nn.Linear(1296, num_classes)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        hidden1 = self.Conv1(x)\n",
    "        relu1 = self.relu(hidden1)\n",
    "        hidden2 = self.Conv2(relu1)\n",
    "        relu2 = self.relu(hidden2)\n",
    "        pool = self.MaxPool(relu2)\n",
    "        hidden3 = self.Conv3(pool)\n",
    "        flattened = hidden3.view(96, -1)\n",
    "        output = self.prediction_layer(flattened)\n",
    "        return output\n",
    "\n",
    "# initialize your model\n",
    "model_cnn_3 = cnn_3(784 ,10)\n",
    "if torch.cuda.is_available():\n",
    "    model_cnn_3 = model_cnn_3.cuda()\n",
    "\n",
    "\n",
    "# create loss: use cross entropy loss\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "# create optimizer\n",
    "optimizer = torch.optim.Adam(model_cnn_3.parameters(), lr=0.001)\n",
    "# transfer your model to train mode\n",
    "model_cnn_3.train()\n",
    "\n",
    "\n",
    "cnn_3_dict = {\"name\":\"cnn_3\", \"loss_curve\": [],  \"train_acc_curve\": [], \"val_acc_curve\": [], \"test_acc\": 0, 'weights': []}\n",
    "\n",
    "\n",
    "\n",
    "# train the model and save the training loss and validation loss for every 10 batches using model eval mode.s\n",
    "for epoch in tqdm(range(15)):\n",
    "    for batch, (x, y) in enumerate(train_generator):\n",
    "        # forward pass\n",
    "        output = model_cnn_3(x)\n",
    "        # compute loss\n",
    "        loss_val = loss(output, y)\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # backward pass\n",
    "        loss_val.backward()\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "        # print loss\n",
    "        if batch % 10 == 0:\n",
    "            model_cnn_3.eval()\n",
    "            #print('Epoch: {}, Batch: {}, Loss: {}'.format(epoch, batch, loss_val.item()))\n",
    "            cnn_3_dict['loss_curve'].append(loss_val.item())\n",
    "            training_accuracy = torch.mean((torch.argmax(output, dim=1) == y).float())\n",
    "            cnn_3_dict['train_acc_curve'].append(training_accuracy.item())\n",
    "\n",
    "            # validation loss   \n",
    "            validation_accuracy_per_batch = torch.tensor([])\n",
    "            with torch.no_grad():\n",
    "                for val_x, val_y in val_generator:\n",
    "                    val_output = model_cnn_3(val_x)\n",
    "                    #val_loss = loss(val_output, val_y)\n",
    "                    val_accuracy = torch.mean((torch.argmax(val_output, dim=1) == val_y).float())\n",
    "                    try:\n",
    "                        validation_accuracy_per_batch = torch.cat((validation_accuracy_per_batch ,val_accuracy))\n",
    "                    except:\n",
    "                        validation_accuracy_per_batch = val_accuracy\n",
    "            #print('Validation Accuracy: {}'.format(validation_accuracy_per_batch.mean()))\n",
    "            cnn_3_dict['val_acc_curve'].append(validation_accuracy_per_batch.mean().item())\n",
    "            model_cnn_3.train()\n",
    "\n",
    "\n",
    "\n",
    "# test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "for x, y in test_generator:\n",
    "    output = model_cnn_3(x)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    total += y.size(0)\n",
    "    correct += (predicted == y).sum().item()\n",
    "print('Test Accuracy: {} %'.format(100 * correct / total))\n",
    "cnn_3_dict['test_acc'] = 100 * correct / total\n",
    "# save the model as a pty file\n",
    "torch.save(model_cnn_3.state_dict(), 'q3_models/model_cnn_3.pty')\n",
    "print(\"model saved as 'q3_models/model_cnn_3.pty'\")\n",
    "# get the parameters 784x32 layer as numpy array\n",
    "weights_first_layer = model_cnn_3.Cnn1.weight.data.numpy()\n",
    "cnn_3_dict['weights'] = weights_first_layer\n",
    "\n",
    "# save cnn_3 as a pickle file\n",
    "import pickle\n",
    "with open('q3_models/cnn_3_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(cnn_3_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5644cb88",
   "metadata": {},
   "source": [
    "### CNN 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879bb83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "# Load fashion MNIST dataset\n",
    "import torchvision\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#training set\n",
    "train_data = torchvision.datasets.FashionMNIST(root='./data', train=True, download=False, transform= torchvision.transforms.ToTensor())\n",
    "\n",
    "#test set\n",
    "test_data = torchvision.datasets.FashionMNIST(root='./data', train=False, download=False, transform= torchvision.transforms.ToTensor())\n",
    "\n",
    "\n",
    "import torch\n",
    "# divide training data into training and validation sets of 0.8 and 0.2 respectively\n",
    "\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [0.8, 0.2])\n",
    "\n",
    "train_generator = torch.utils.data.DataLoader(train_data, batch_size=96, shuffle=True)\n",
    "val_generator = torch.utils.data.DataLoader(val_data, batch_size=96, shuffle=False)\n",
    "test_generator = torch.utils.data.DataLoader(test_data, batch_size=96, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# example cnn_4 classifier\n",
    "class cnn_4(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(cnn_4, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.Conv1 = torch.nn.Conv2d(1 ,16 ,3, stride=1, padding=1)\n",
    "        self.Conv2 = torch.nn.Conv2d(16, 8, 3,  stride=1, padding=1)\n",
    "        self.Conv3 = torch.nn.Conv2d(8, 16, 5, stride=1, padding=1)\n",
    "        self.MaxPool = torch.nn.MaxPool2d(2, stride=2)\n",
    "        self.Conv4 = torch.nn.Conv2d(16, 16, 5,  stride=1, padding=1)\n",
    "        self.prediction_layer = torch.nn.Linear(400, num_classes)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        hidden1 = self.Conv1(x)\n",
    "        relu1 = self.relu(hidden1)\n",
    "        hidden2 = self.Conv2(relu1)\n",
    "        relu2 = self.relu(hidden2)\n",
    "        hidden3 = self.Conv3(relu2)\n",
    "        relu3 = self.relu(hidden3)\n",
    "        pool1 = self.MaxPool(relu3)\n",
    "        hidden4 = self.Conv4(pool1)\n",
    "        pool2 = self.MaxPool(hidden4)\n",
    "        flattened = pool2.view(96, -1)\n",
    "        output = self.prediction_layer(flattened)\n",
    "        return output\n",
    "\n",
    "# initialize your model\n",
    "model_cnn_4 = cnn_4(784 ,10)\n",
    "if torch.cuda.is_available():\n",
    "    model_cnn_4 = model_cnn_4.cuda()\n",
    "\n",
    "\n",
    "# create loss: use cross entropy loss\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "# create optimizer\n",
    "optimizer = torch.optim.Adam(model_cnn_4.parameters(), lr=0.001)\n",
    "# transfer your model to train mode\n",
    "model_cnn_4.train()\n",
    "\n",
    "\n",
    "cnn_4_dict = {\"name\":\"cnn_4\", \"loss_curve\": [],  \"train_acc_curve\": [], \"val_acc_curve\": [], \"test_acc\": 0, 'weights': []}\n",
    "\n",
    "\n",
    "\n",
    "# train the model and save the training loss and validation loss for every 10 batches using model eval mode.s\n",
    "for epoch in tqdm(range(15)):\n",
    "    for batch, (x, y) in enumerate(train_generator):\n",
    "        # forward pass\n",
    "        output = model_cnn_4(x)\n",
    "        # compute loss\n",
    "        loss_val = loss(output, y)\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # backward pass\n",
    "        loss_val.backward()\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "        # print loss\n",
    "        if batch % 10 == 0:\n",
    "            model_cnn_4.eval()\n",
    "            #print('Epoch: {}, Batch: {}, Loss: {}'.format(epoch, batch, loss_val.item()))\n",
    "            cnn_4_dict['loss_curve'].append(loss_val.item())\n",
    "            training_accuracy = torch.mean((torch.argmax(output, dim=1) == y).float())\n",
    "            cnn_4_dict['train_acc_curve'].append(training_accuracy.item())\n",
    "\n",
    "            # validation loss   \n",
    "            validation_accuracy_per_batch = torch.tensor([])\n",
    "            with torch.no_grad():\n",
    "                for val_x, val_y in val_generator:\n",
    "                    val_output = model_cnn_4(val_x)\n",
    "                    #val_loss = loss(val_output, val_y)\n",
    "                    val_accuracy = torch.mean((torch.argmax(val_output, dim=1) == val_y).float())\n",
    "                    try:\n",
    "                        validation_accuracy_per_batch = torch.cat((validation_accuracy_per_batch ,val_accuracy))\n",
    "                    except:\n",
    "                        validation_accuracy_per_batch = val_accuracy\n",
    "            #print('Validation Accuracy: {}'.format(validation_accuracy_per_batch.mean()))\n",
    "            cnn_4_dict['val_acc_curve'].append(validation_accuracy_per_batch.mean().item())\n",
    "            model_cnn_4.train()\n",
    "\n",
    "\n",
    "\n",
    "# test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "for x, y in test_generator:\n",
    "    output = model_cnn_4(x)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    total += y.size(0)\n",
    "    correct += (predicted == y).sum().item()\n",
    "print('Test Accuracy: {} %'.format(100 * correct / total))\n",
    "cnn_4_dict['test_acc'] = 100 * correct / total\n",
    "# save the model as a pty file\n",
    "torch.save(model_cnn_4.state_dict(), 'q3_models/model_cnn_4.pty')\n",
    "print(\"model saved as 'q3_models/model_cnn_4.pty'\")\n",
    "# get the parameters 784x32 layer as numpy array\n",
    "weights_first_layer = model_cnn_4.Cnn1.weight.data.numpy()\n",
    "cnn_4_dict['weights'] = weights_first_layer\n",
    "\n",
    "# save cnn_4 as a pickle file\n",
    "import pickle\n",
    "with open('q3_models/cnn_4_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(cnn_4_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8d2dfc",
   "metadata": {},
   "source": [
    "### CNN 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "# Load fashion MNIST dataset\n",
    "import torchvision\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#training set\n",
    "train_data = torchvision.datasets.FashionMNIST(root='./data', train=True, download=False, transform= torchvision.transforms.ToTensor())\n",
    "\n",
    "#test set\n",
    "test_data = torchvision.datasets.FashionMNIST(root='./data', train=False, download=False, transform= torchvision.transforms.ToTensor())\n",
    "\n",
    "\n",
    "import torch\n",
    "# divide training data into training and validation sets of 0.8 and 0.2 respectively\n",
    "\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [0.8, 0.2])\n",
    "\n",
    "train_generator = torch.utils.data.DataLoader(train_data, batch_size=96, shuffle=True)\n",
    "val_generator = torch.utils.data.DataLoader(val_data, batch_size=96, shuffle=False)\n",
    "test_generator = torch.utils.data.DataLoader(test_data, batch_size=96, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# example cnn_5 classifier\n",
    "class cnn_5(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(cnn_5, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.Conv1 = torch.nn.Conv2d(1 ,8 ,3, stride=1, padding=1)\n",
    "        self.Conv2 = torch.nn.Conv2d(8, 16, 3,  stride=1, padding=1)\n",
    "        self.Conv3 = torch.nn.Conv2d(16, 8, 3, stride=1, padding=1)\n",
    "        self.Conv4 = torch.nn.Conv2d(8, 16, 3,  stride=1, padding=1)\n",
    "        self.MaxPool = torch.nn.MaxPool2d(2, stride=2)\n",
    "        self.Conv5 = torch.nn.Conv2d(16, 16, 3,  stride=1, padding=1)\n",
    "        self.Conv6 = torch.nn.Conv2d(16, 8, 3,  stride=1, padding=1)\n",
    "        self.prediction_layer = torch.nn.Linear(392, num_classes)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        hidden1 = self.Conv1(x)\n",
    "        relu1 = self.relu(hidden1)\n",
    "        hidden2 = self.Conv2(relu1)\n",
    "        relu2 = self.relu(hidden2)\n",
    "        hidden3 = self.Conv3(relu2)\n",
    "        relu3 = self.relu(hidden3)\n",
    "        hidden4 = self.Conv4(relu3)\n",
    "        relu4 = self.relu(hidden4)\n",
    "        pool1 = self.MaxPool(relu4)\n",
    "        hidden5 = self.Conv5(pool1)\n",
    "        relu5 = self.relu(hidden5)\n",
    "        hidden6 = self.Conv6(relu5)\n",
    "        relu6 = self.relu(hidden6)\n",
    "        pool2 = self.MaxPool(relu6)\n",
    "        flattened = pool2.view(96, -1)\n",
    "        output = self.prediction_layer(flattened)\n",
    "        return output\n",
    "\n",
    "# initialize your model\n",
    "model_cnn_5 = cnn_5(784 ,10)\n",
    "if torch.cuda.is_available():\n",
    "    model_cnn_5 = model_cnn_5.cuda()\n",
    "\n",
    "\n",
    "# create loss: use cross entropy loss\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "# create optimizer\n",
    "optimizer = torch.optim.Adam(model_cnn_5.parameters(), lr=0.001)\n",
    "# transfer your model to train mode\n",
    "model_cnn_5.train()\n",
    "\n",
    "\n",
    "cnn_5_dict = {\"name\":\"cnn_5\", \"loss_curve\": [],  \"train_acc_curve\": [], \"val_acc_curve\": [], \"test_acc\": 0, 'weights': []}\n",
    "\n",
    "\n",
    "\n",
    "# train the model and save the training loss and validation loss for every 10 batches using model eval mode.s\n",
    "for epoch in tqdm(range(15)):\n",
    "    for batch, (x, y) in enumerate(train_generator):\n",
    "        # forward pass\n",
    "        output = model_cnn_5(x)\n",
    "        # compute loss\n",
    "        loss_val = loss(output, y)\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # backward pass\n",
    "        loss_val.backward()\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "        # print loss\n",
    "        if batch % 10 == 0:\n",
    "            model_cnn_5.eval()\n",
    "            #print('Epoch: {}, Batch: {}, Loss: {}'.format(epoch, batch, loss_val.item()))\n",
    "            cnn_5_dict['loss_curve'].append(loss_val.item())\n",
    "            training_accuracy = torch.mean((torch.argmax(output, dim=1) == y).float())\n",
    "            cnn_5_dict['train_acc_curve'].append(training_accuracy.item())\n",
    "\n",
    "            # validation loss   \n",
    "            validation_accuracy_per_batch = torch.tensor([])\n",
    "            with torch.no_grad():\n",
    "                for val_x, val_y in val_generator:\n",
    "                    val_output = model_cnn_5(val_x)\n",
    "                    #val_loss = loss(val_output, val_y)\n",
    "                    val_accuracy = torch.mean((torch.argmax(val_output, dim=1) == val_y).float())\n",
    "                    try:\n",
    "                        validation_accuracy_per_batch = torch.cat((validation_accuracy_per_batch ,val_accuracy))\n",
    "                    except:\n",
    "                        validation_accuracy_per_batch = val_accuracy\n",
    "            #print('Validation Accuracy: {}'.format(validation_accuracy_per_batch.mean()))\n",
    "            cnn_5_dict['val_acc_curve'].append(validation_accuracy_per_batch.mean().item())\n",
    "            model_cnn_5.train()\n",
    "\n",
    "\n",
    "\n",
    "# test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "for x, y in test_generator:\n",
    "    output = model_cnn_5(x)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    total += y.size(0)\n",
    "    correct += (predicted == y).sum().item()\n",
    "print('Test Accuracy: {} %'.format(100 * correct / total))\n",
    "cnn_5_dict['test_acc'] = 100 * correct / total\n",
    "# save the model as a pty file\n",
    "torch.save(model_cnn_5.state_dict(), 'q3_models/model_cnn_5.pty')\n",
    "print(\"model saved as 'q3_models/model_cnn_5.pty'\")\n",
    "# get the parameters 784x32 layer as numpy array\n",
    "weights_first_layer = model_cnn_5.Cnn1.weight.data.numpy()\n",
    "cnn_5_dict['weights'] = weights_first_layer\n",
    "\n",
    "# save cnn_5 as a pickle file\n",
    "import pickle\n",
    "with open('q3_models/cnn_5_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(cnn_5_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0da5c75-9844-4660-87f1-2a8452f0aecc",
   "metadata": {},
   "source": [
    "## Experimenting Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e710d8-a707-44ad-b663-e8ca151e4fe6",
   "metadata": {},
   "source": [
    "## Experimenting Learning Rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
